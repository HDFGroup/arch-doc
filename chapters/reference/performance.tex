
%|  Name  | TODO | ONGOING | DONE |
%|--------|------|---------|------|
%| Dana   | x    |         |      |
%| Gerd   | x    |         |      |
%| Glenn  |      |    x    |      |
%| Jordan | x    |         |      |
%| Luke   | x    |         |      |
%| Matt   | x    |         |      |
%| Neil   | x    |         |      |
%| Scot   | x    |         |      |


%\todo[inline]{Owner: Neil -- Priority: High -- Effort: M -- Completion: 100\%}

\paragraph{Scope.}

\begin{itemize}
%    \item Library dos and dont's
    \item Skip list \& hash table use

The skip list is a simple in-memory data structure that allows O(log N) key/value insertion, lookup, and removal. Skip lists are used widely in the library for many tasks, including property list operations and I/O chunk selection (among many others). While skip lists should theoretically be efficient for these use cases as long as N does not grow extremely large, profiling has found that skip list operations are a frequent cause of library underperformance, even though the N values (number of properties, number of IDs) are not extremely large. We currently suspect this underperformance is due to the shape of the skip list data structures causing frequent CPU cache misses. More investigation is needed, and it may be possible to improve skip list performance with additional work. It is now recommended to avoid using the skip list in performance-critical sections of the code wherever possible. The primary motivation behind the context package (\texttt{H5CX}) is to avoid invoking skip list operations when handling property lists whenever possible. Unlike a classic skip list, HDF5's skip list is deterministic and does not use the random number generator and, therefore, will not interfere with the application's random numbers.

In order to improve performance, we have more recently incorporated the open-source uthash \cite{uthash} hash table into HDF5 and begun using it in places that previously would have used the skip list. A hash table offers, in general, O(1) insertion, lookup, and removal at the cost of higher memory usage. Therefore, a hash table can be expected to scale better than a skip list when handling large numbers of objects (again, at the cost of high memory usage). In practice, we have also found uthash to be more performant even with smaller numbers of objects. We recommend that developers use uthash instead of skip lists whenever possible and whenever memory usage is not critical.

When objects inserted into these structures need to be iterated over in key order, the advantage of uthash is less apparent. In this case, items in the skip list are already in the correct order, but the uthash hash table must be sorted with \texttt{HASH\_SORT}, which is an O(log N) operation. Likely, uthash is still faster in this case but needs more investigation.

    \item Allocations and deallocations

In order to minimize the number of calls to system memory management functions, the HDF5 library has implemented the free list (\texttt{H5FL}) package. This package serves as a cache of allocated memory blocks and is effectively an intermediate memory manager between the library and the operating system. Regular free lists are the most commonly used type of free list and serve as a cache of allocations of a single structure type. Other free list types are sequence, array, and factory free lists that are used for variable-sized allocations and are less frequently used, and may not be as efficient as simply using the system memory manager. More investigation is needed.

We have found mixed results when evaluating the performance of the free lists in HDF5. There is a configure option to disable free lists, sometimes leading to a greater performance with free lists disabled. It is likely that, when performing \texttt{calloc} operations, free lists are less efficient than simply using the system \texttt{calloc()} since the free list must \texttt{memset} the buffer, while the system can return a block from a page it knows has already been set to zero. For this reason, we recommend avoiding the use of \texttt{H5FK\_CALLOC()} whenever possible, and when that is not possible, consider using the system \texttt{calloc()} instead, unless there is reason to believe the free list will be faster. Note that, since you cannot mix free list and system memory management calls on the same memory block, if \texttt{calloc} is only needed some of the time, it might require profiling to determine which method is best.

    \item Parallel performance considerations

Parallel access can significantly improve raw data performance when HDF5 is used on a parallel file system. However, there are additional things to keep in mind to maximize performance and avoid falling into traps. For raw data, the most important decision to make is collective vs. independent I/O. While independent I/O gives more flexibility to the application, since it does not need to be called by every rank, collective I/O often gives better performance. This is because it allows MPI to intelligently distribute data in the most efficient manner possible while coordinating with other ranks. However, sometimes the overhead associated with collaboration, and the overhead associated with creating and unpacking the MPI datatypes, outweighs the performance benefits. It is difficult to tell which method will be faster without benchmarking, so it is a good idea to try independent, collective, and collective interface with independent low-level I/O (as set by \func{H5Pset_dxpl_mpio_collective_opt}).

The shape of the I/O can also have a substantial impact on raw data I/O performance. While the MPI datatype facility (used when called with the collective interface) should allow more efficient I/O with regularly strided selections because it allows such a pattern to be passed to MPI in a single I/O call, in practice it is still faster to use contiguous blocks for I/O. This often also extends even to very large blocks, where it is more efficient for each rank's chunks or blocks to be placed next to each other than to have the blocks from the different ranks interleaved.

The application also has an option to use independent or collective I/O for metadata operations. Since all metadata write operations are required to be called collectively in HDF5, enabling metadata writes with \func{H5Pset_coll_metadata_write} does not affect the semantics of the operation, and the library still coordinates even if metadata writes are independent. Collective metadata writes often improve performance but it is worth checking to see which option works best for each application. In either case each piece of metadata is only written once.

Enabling metadata reads with \func{H5Pset_all_coll_metadata_ops} changes the semantics of HDF5, requiring that all operations that read (or write) metadata be called collectively. This imposes an additional burden on the application, but in some cases can lead to substantial performance improvements. For example, if every process is opening the same dataset, then with the default independent metadata reads, every process issues its own read call and the filesystem is immediately swamped with N (number of ranks) read requests. However, when collective metadata reads are turned on, the filesystem only sees a single read request, with MPI internally distributing the data to all ranks in a much more efficient manner. These sorts of metadata read storms are a common cause of unexpected slowdowns in parallel code and should be among the first things to check for when diagnosing performance problems in parallel.

When creating new library code for parallel HDF5, developers must always consider what the most efficient use of MPI is. Coordination/communication is often much more expensive than computation on a single rank, so effort must be put in to minimize MPI calls that trigger network access, and to use the most efficient versions of these calls whenever possible. For example, don't call \func{MPI_Allgather} when \func{MPI_Gather} would suffice. In addition, it is often more efficient to compute the same values on every rank then to compute it once and broadcast the result. This can be seen in multiple places in the metadata cache and the I/O pipeline. Finally, the developer may need to balance memory usage against performance, since in some cases the memory available to each rank can be small (if there are many ranks per node) and things like MPI datatypes can use a substantial amount of memory. It may be necessary to add an API option to optimize for memory usage or performance, as is the case with link-chunk vs multi-chunk I/O.

\end{itemize}
